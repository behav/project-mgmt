---
title: "Meeting notes"
author: "Rick O. Gilmore"
date: "`r Sys.time()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 2019-07-31, BehAV.ai + Jeff Spies

## Agenda

1. Discuss Dan's findings on video annotation tools
2. Discuss next steps for video analytics benchmarking project

## Action items

1. Dan will think about what research questions in his area of expertise, especially facial expression recognition, seem most apt as a demonstration case.
2. Dan will try to get Microsoft's VoTT up to demo full functionality
3. Depending on 2., Jeff may help us contact the MS group

# 2019-07-31, BehAV.ai team

## Attendees

- Rick, Dan, Drew, Kory

## Agenda

1. Update on annotation tools functions on HPC
    - Review of labeling tools, combine annotations (Dan); focus on [LabelMe](https://github.com/wkentaro/labelme)
        - Kory found an annotation tool; will send Dan link
    - Convert output from existing annotation programs
        - Bounding boxes & polygons easy; pose estimation (e.g., OpenPose) harder because data representation has more points, segment connections
2. Discuss optimization of existing workflows
3. Discuss video benchmarking
4. Discuss integration/UI/UX for multiple annotations

## Action items

1. Rick describe full stack workflow
2. Drew will evaluate and recommend strategies to optimize on existing hardware
3. Drew and Dan will recommend software upgrades to request from ICS
4. Drew & Kory will get info about <https://www.ibm.com/us-en/marketplace/power-systems-ac922/details> mini-Summit node that PSU is bringing online.
5. Rick will email Wayne & Jenni about early access for testing (also discuss software config re: NVIDIA drivers, etc.)
6. Kory will send Dan info about IBM and other annotation tools

# 2019-07-24, Rick & Dan

## Agenda

1. Update on annotation tools
2. Discuss next steps

## Action steps

1. Rick will try to schedule team meeting before 8/2 or after 8/12.
2. Rick will schedule meeting with Dan and Jeff Spies to discuss annotation tool findings.
3. Dan will evaluate the work required to convert OpenFace, FER, and YOLO annotations into common (LabelMe) format for comparison.

# 2019-06-28, Video criteria for QA and comparative analyses

## Dimensions for video variability

1. Setting: indoor {lab, home, hallway}, outdoor
2. Number of people in view
3. Number, diversity of objects in view
4. Animals in view
5. Any pictures of humans or human-shape-like in view?
6. Rough estimate of angle between the human face and the camera (front, sideways, front/sideways, back)
7. Size (approximate proportion of field of view) taken up by {face, hands, whole bodies}
8. Overall level of illumination {bright, medium, dark}
9. Camera position {fixed, in-motion}
10. Video resolution (in pixels)
11. Video resolution (in temporal sampling or frame rate)
12. Video colormap: grayscale or RGB color
13. Original recording type (analog/VHS, digital)
14. Accompanying Datavyu codes that relate to emotional expression or some type of physical movement that could have a distinctive body pose pattern

## Dimensions/variables for video evaluation

1. Proportion of video frames where X {face, object, pose} was detected successfully
2. Proportion of frames that match judgment of human coder
3. Cross correlation between models fitting same underlying contruct (e.g., detecting faces, labeling body segments)

# 2019-06-28, Rick & Ori

## Agenda

- QA planning, what videos, what variables to evaluate/compare
- UI for visualizing & editing annotations

## Action items

- Rick will brainstorm a list of variables along which we want videos to vary
- Rick will brainstorm a list of measures we want to use to evaluate/compare the videos
- Ori will modify & extend Rick's drafts
- Once we have specs for videos, Rick & Ori will talk with Karen and then the lab experts to enlist them in helping us find suitable videos.

# 2019-06-28 Rick & Dan

## Agenda

- Discuss Dan's recent work on "containerizing" various models, e.g., YOLO
- Discuss Dan's findings about what software versions are needed on ACI to optimize our use of the latest algorithms
- Discuss how to deal with idiosyncrasies in data output formats from different models
- Discuss Rick's suggestion that VoTT might be a suitable base for building a visualization tool

## Action items

- Rick will talk with Ori about the QA process (see above), including providing access to test videos
- Dan will spend some time evaluating VoTT

# 2019-06-17,  NYU Team re: non-human animal data, Zoom

## Attendees

- Rick Gilmore, Alisa Surkis, Kevin Read, Rob Froemke, Jeff Spies, Peter Petersen

## Agenda

- Discuss NIH Brain Initiative calls, especially [RFA-MH-19-147](https://grants.nih.gov/grants/guide/rfa-files/RFA-MH-19-147.html) and [RFA-MH-19-145](https://grants.nih.gov/grants/guide/rfa-files/RFA-MH-19-145.html).
- Background email from Ming Zhan at NIMH:

From: Zhan, Ming (NIH/NIMH) [E] <ming.zhan@nih.gov>
Sent: Friday, April 26, 2019 4:52 PM
To: rog1@psu.edu
Cc: Gnadt, Jim (NIH/NINDS) [E]; David, Karen (NIH/NINDS) [E]; Farber, Greg (NIH/NIMH) [E]; Karen Adolph; Froemke, Robert; Jeff Spies
Subject: RE: Meet tomorrow (Sat) @ BRAIN initiative mtg?
 
Hi Rick,
 
Thanks for your interests in the BRAIN Initiative Informatics Program.
 
We had a discussion about your option of either developing a data archive or a data analysis software for your potential application, and considered that a project of software development would better fit to our program. I would therefore suggest you submit an application in response to MH-19-147.
 
I expect that the major users of the to-be-developed software would be investigators from the BRAIN U19 program. It would be great if you could interact with those investigators in your preparation of the application. I’ve also cc’ed to Drs. Jim Gnadt and Karen David, the POs of the program, who may send you contact persons or additional information of the program.
 
Best,
Ming
 
Ming Zhan, Ph.D.
NIH BRAIN Initiative Informatics Program
Office of Technology Development and Coordination
National Institute of Mental Health, NIH
6001 Executive Blvd,  Room 8152
Rockville MD 20852, USA
Phone: (301) 827-3678
E-mail:  ming.zhan@nih.gov
https://www.braininitiative.nih.gov/brain-programs/informatics
 
 
From: Gilmore, Rick Owen <rog1@psu.edu> 
Sent: Saturday, April 20, 2019 8:08 PM
To: Zhan, Ming (NIH/NIMH) [E] <ming.zhan@nih.gov>
Cc: Farber, Greg (NIH/NIMH) [E] <farberg@mail.nih.gov>; Karen Adolph <kea1@nyu.edu>; Froemke, Robert <Robert.Froemke@nyulangone.org>; Jeff Spies <jeff@221b.io>
Subject: Re: Meet tomorrow (Sat) @ BRAIN initiative mtg?
 
Dear Dr. Zhan,
 
As promised, here are some bullet points describing the ideas we have been discussing. We look forward to talking with you about them.
 
- Build upon and enhance the NIH-funded Databrary.org video data repository's storage, streaming, & sharing capacity.
- Extend Databrary.org restricted access policy framework to include permitting restricted but open access to videos of non-human animals.
- Extend the scope of searchable metadata to incorporate standard fields most useful for data discovery and reuse among the BRAIN initiative and neuroscience research communities.
- Enhance Databrary's infrastructure to enable storage of other temporally-dense data files that accompany video streams (e.g, kinematics, physiology).
 
Data analysis tool-related ideas
 
- Incorporate computer vision-enhanced annotation (object labeling, pose/motion tracking) tools into the Databrary system so that other researchers can easily deploy these tools on shared videos.
- Extend the Databrary data model to allow human- and machine-produced annotations and other time series data streams to be visualized in the browser.
- Create ways to store time series data streams in common, open, and easily combined formats on Databrary to support the construction, storage, visualization, and sharing of multiple annotation layers.
- Enhance the Databrary API's existing capacity to find, filter, and serve specified labeled time segments of video or audio to make it possible for users to create training and testing data sets for custom computer vision-enhanced workflows.
 
I’ve copied BRAIN Initiative investigator, Rob Froemke, and my Databrary colleagues, Karen Adolph and Jeff Spies.
 
Best,
 
Rick Gilmore
 
Rick O. Gilmore, Ph.D.
Associate Professor of Psychology
Co-Director, Databrary.org
114 Moore Building
The Pennsylvania State University
University Park, PA 16801
814-865-3664 (voice)
rog1@psu.edu
gilmore-lab.github.io

## Action items

- Rob will talk with DeepLabCut developers soon.
- Rob will consider talking with European group that is also using video.
- Rick will draft and circulate a specific aims page.
- Peter will share information about the metadata schema his group is developing.
- Everyone will read [RFA-MH-19-147](https://grants.nih.gov/grants/guide/rfa-files/RFA-MH-19-147.html) carefully and think about PI/Co-PI roles. Rick has agreed to serve as PI, with Rob as Co-PI.
- Rick will poll the group about availability to meet again the week of 8/19 or 8/26.
- Rick will circulate some relevant draft proposals and reviews.
 
# 2019-05-30, 350 Moore Building

## Attendees

- Rick Gilmore, Kory Blose (remote), Dan Albohn, Drew Polasky, Ori Ossmy (remote)

## Agenda

1. Introductions
2. Update from Dan about emotion processing workflow [html](https://behav.github.io/project-mgmt/behAV_example_051419.html).
3. Building on Dan's work, specifically optimization (Drew)

## Next steps

- Drew will work with Dan on optimizing the code for i) interactive use, ii) use of GPU nodes, and iii) eventually batch processing. Focus is specifically on optimizing OpenPose since it is so slow.
- Dan will containerize YOLO.
- Kory will provide Dan some videos; Dan, Rick, and Ori will also look for videos with more diverse components (faces, objects, PLAY videos Ori has been running OpenPose + YOLO on)
- Rick, Ori, & Dan will develop a QA protocol, focusing first on # of frames with faces detected, and expanding to other variables later. The goal is to try to identify where the model(s) have particular difficulty and the features of those frames.
- Drew will set-up 5TB group allocation.

# 2019-05-01, IST Bridge

## Attendees

- Steve Fast, Kory Blose, Dan Albohn, Rick Gilmore, ~~Ori Ossmy (remote)~~ 

## Agenda

1. Introductions
2. Updates
    - IBM workshop and resources (Kory)
    - NIH BRAIN initiative (Rick)
3. Goal-setting
    - Deploy X video/image analysis tools on HPC
        - Which tools? Cloud (AI as a Service), open source, or?
        - Which tools? What use-cases?
    - Create workflows for video/image analysis suitable for use by non-specialists
    - Integration with Databrary or other API-accessible data sources

## Action items

1. Kory and Dan will meet; create milestones.
2. Rick to ACI about RA (Drew Polasky) and space/group account.
