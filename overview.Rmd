---
title: "Overview"
author: "Rick O. Gilmore"
date: "`r Sys.time()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose

To describe the BehAV.ai project team, resources available, and goals of the effort.

# Support

## Active

BehAV.ai is supported by the Penn State Institute for Cyberscience (ICS) seed grant program.

## Potential

- <https://grants.nih.gov/grants/guide/rfa-files/RFA-MH-19-145.html>

# Team members

- Dan Albohn, M.S., PSU (developer)
- Kory Blose, Ph.D., PSU/ARL (Co-PI)
- Stephen Fast, Ph.D., PSU/ARL (Co-PI)
- Rick Gilmore, Ph.D., PSU/Databrary.org (PI)
- Ori Ossmy, Ph.D., NYU (consultant)
- Jeff Spies, Ph.D. Databrary.org (adviser)
- Conrad Tucker, Ph.D., PSU/CMU (Co-PI)
- Drew Polasky, PSU/ICS (consultant)

# Resources

## Technology evaluation

### Domains

Most tools focus on a small subset of domains:

1. Object/face detection or labeling
2. Body pose estimation/motion tracking
3. Speech transcription

Pliers, on the other hand, provides a set of tools to generate annotations in a common data format.

### Software stack

Pliers, DeepLabCut, and LEAP are Python tools. VoTT uses node.js.

## Open source software tools

- [Microsoft Visual Object Tagging Tool (VoTT)](https://github.com/Microsoft/VoTT)
- [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) pose estimation
- [DeepLapCut](https://github.com/AlexEMG/DeepLabCut) motion tracking, especially for non-human animals
- [Leap Estimates Animal Pose (LEAP)](https://github.com/talmo/leap)
    - ROG met the developer Talmo Pereira at the 2019 BRAIN Initiative investigators meeting.
- [JAABA - Janelia Automatic Animal Behavior Annotator](http://jaaba.sourceforge.net/)
- [Pliers](https://github.com/tyarkoni/pliers), Python package for integrating video/audio/image feature extraction
- [You Only Look Once (Yolo)](https://pjreddie.com/darknet/yolo/) object labeling
- [OpenCV](https://opencv.org/)

## References

Biggs, B., Roddick, T., Fitzgibbon, A., & Cipolla, R. (2018, November 14). Creatures great and SMAL: Recovering the shape and motion of animals from video. arXiv [cs.CV]. Retrieved from arXiv.

Clemens, J., Coen, P., Roemschied, F. A., Pereira, T. D., Mazumder, D., Aldarondo, D. E., Pacheco, D. A., et al. (2018). Discovery of a New Song Mode in Drosophila Reveals Hidden Structure in the Sensory and Neural Drivers of Behavior. Current biology: CB, 28(15), 2400–2412.e6. Elsevier. Retrieved from http://dx.doi.org/10.1016/j.cub.2018.06.011

Francisco, F. A., Nührenberg, P., & Jordan, A. L. (2019). A low-cost, open-source framework for tracking and behavioural analysis of animals in aquatic ecosystems. bioRxiv. biorxiv.org. Retrieved from https://www.biorxiv.org/content/10.1101/571232v1.abstract

Mathis, A., Mamidanna, P., Cury, K. M., Abe, T., Murthy, V. N., Mathis, M. W., & Bethge, M. (2018). DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. Nature neuroscience, 21(9), 1281–1289. Retrieved from http://dx.doi.org/10.1038/s41593-018-0209-y

Pereira, T. D., Aldarondo, D. E., Willmore, L., Kislin, M., -H. Wang, S. S., Murthy, M., & Shaevitz, J. W. (2018, May 30). Fast animal pose estimation using deep neural networks. bioRxiv. Retrieved April 13, 2019, from https://www.biorxiv.org/content/10.1101/331181v2.abstract

Štih, V., Petrucco, L., Kist, A. M., & Portugues, R. (2019). Stytra: An open-source, integrated system for stimulation, tracking and closed-loop behavioral experiments. PLoS computational biology, 15(4), e1006699. journals.plos.org. Retrieved from http://dx.doi.org/10.1371/journal.pcbi.1006699

Wei, K., & Kording, K. P. (2018). Behavioral tracking gets real. Nature neuroscience, 21(9), 1146–1147. Retrieved from http://dx.doi.org/10.1038/s41593-018-0215-0
