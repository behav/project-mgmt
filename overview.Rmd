---
title: "Overview"
author: "Rick O. Gilmore"
date: "`r Sys.time()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose

To describe the BehAV.ai project team, resources available, and goals of the effort.

# Support

## Active

BehAV.ai is supported by the Penn State Institute for Cyberscience (ICS) seed grant program.

- [Proposal narrative](https://docs.google.com/document/d/1XwKC_F5byeeSgrrgL2t8wT7J4DDVVST1jY1Rdmr7NNg/edit?usp=sharing)
- [Proposal abstract](https://docs.google.com/document/d/13pungK4Vi6aOHCBQxBCmrkHJ3uUsyhgdRWLOzFyFHeg/edit?usp=sharing)

## Potential

- <https://grants.nih.gov/grants/guide/rfa-files/RFA-MH-19-145.html>

# Team members

- Dan Albohn, M.S., PSU (developer)
- Kory Blose, Ph.D., PSU/ARL (Co-PI)
- Stephen Fast, Ph.D., PSU/ARL (Co-PI)
- Rick Gilmore, Ph.D., PSU/Databrary.org (PI)
- Ori Ossmy, Ph.D., NYU (consultant)
- Jeff Spies, Ph.D. Databrary.org (adviser)
- Conrad Tucker, Ph.D., PSU/CMU (Co-PI)
- Drew Polasky, PSU/ICS (consultant)

# Goals

1. Implement video/image data analytic workflows on PSU HPC resources
2. Create UI to enable non-specialist users to deploy these workflows on their own videos
3. Demonstrate utility of using videos stored on Databrary or other API-accessible sources
4. Develop open/extensible video annotation format
5. (Stretch) pilot UI/UX for visualizing annotations from multiple video analysis tools
6. (Stretch) pilot scheme for semi-automated (human-in-the-loop) creation and modification of video analysis workflows

# Resources

## Technology evaluation

### Domains

Most tools focus on a small subset of domains:

1. Object/face detection or labeling
2. Body pose estimation/motion tracking
3. Speech transcription

[Pliers](https://github.com/tyarkoni/pliers), on the other hand, provides a set of tools to generate annotations in a common data format.

### Issues

- Open source vs. proprietary
- Do cloud/AI as a service providers maintain confidentiality/privacy? Some terms of use involve giving unlimited rights to analytic providers.

### Software stack

Pliers, DeepLabCut, and LEAP are Python tools. VoTT uses node.js.

- A resource we may wish to consider is [json-schema](https://json-schema.org/).

## Open source software tools

- [Microsoft Visual Object Tagging Tool (VoTT)](https://github.com/Microsoft/VoTT)
- [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) pose estimation
- [DeepLapCut](https://github.com/AlexEMG/DeepLabCut) motion tracking, especially for non-human animals
- [Leap Estimates Animal Pose (LEAP)](https://github.com/talmo/leap)
      - ROG met the developer, Talmo Pereira, at the 2019 BRAIN Initiative investigators meeting.
- [JAABA - Janelia Automatic Animal Behavior Annotator](http://jaaba.sourceforge.net/)
- [Pliers](https://github.com/tyarkoni/pliers), Python package for integrating video/audio/image feature extraction
- [You Only Look Once (Yolo)](https://pjreddie.com/darknet/yolo/) object labeling
    - ROG's colleague, Chen Yu, uses YOLO in his research.
- [OpenCV](https://opencv.org/)

## Commercial resources

- [IBM PowerAI Vision](https://www.ibm.com/us-en/marketplace/ibm-powerai-vision)
- IBM Intelligent Video Analytics

## Bibliography

Biggs, B., Roddick, T., Fitzgibbon, A., & Cipolla, R. (2018, November 14). Creatures great and SMAL: Recovering the shape and motion of animals from video. arXiv [cs.CV]. Retrieved from arXiv.

Clemens, J., Coen, P., Roemschied, F. A., Pereira, T. D., Mazumder, D., Aldarondo, D. E., Pacheco, D. A., et al. (2018). Discovery of a New Song Mode in Drosophila Reveals Hidden Structure in the Sensory and Neural Drivers of Behavior. Current biology: CB, 28(15), 2400–2412.e6. Elsevier. Retrieved from http://dx.doi.org/10.1016/j.cub.2018.06.011

Francisco, F. A., Nührenberg, P., & Jordan, A. L. (2019). A low-cost, open-source framework for tracking and behavioural analysis of animals in aquatic ecosystems. bioRxiv. biorxiv.org. Retrieved from https://www.biorxiv.org/content/10.1101/571232v1.abstract

Gilmore, R.O. (n.d.). Databraryapi: An R package for interacting with the Databrary API. Retrieved from http://github.com/PLAY-behaviorome/databraryapi.

Mathis, A., Mamidanna, P., Cury, K. M., Abe, T., Murthy, V. N., Mathis, M. W., & Bethge, M. (2018). DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. Nature neuroscience, 21(9), 1281–1289. Retrieved from http://dx.doi.org/10.1038/s41593-018-0209-y

Ossmy, O., Gilmore, R.O., & Adolph, K.E. (in press). AutoViDev: A computer-vision framework to enhance and accelerate research in human development. In Advances in Intelligent Systems and Computing. Presented at the Computer Vision Conference (CVC) 2019, Springer.

Pereira, T. D., Aldarondo, D. E., Willmore, L., Kislin, M., -H. Wang, S. S., Murthy, M., & Shaevitz, J. W. (2018, May 30). Fast animal pose estimation using deep neural networks. bioRxiv. Retrieved April 13, 2019, from https://www.biorxiv.org/content/10.1101/331181v2.abstract

Prakash, S. K. A., & Tucker, C. S. (2018). Bounded Kalman filter method for motion-robust, non-contact heart rate estimation. Biomedical Optics Express, 9(2), 873–897.

Štih, V., Petrucco, L., Kist, A. M., & Portugues, R. (2019). Stytra: An open-source, integrated system for stimulation, tracking and closed-loop behavioral experiments. PLoS computational biology, 15(4), e1006699. journals.plos.org. Retrieved from http://dx.doi.org/10.1371/journal.pcbi.1006699

Wei, K., & Kording, K. P. (2018). Behavioral tracking gets real. Nature neuroscience, 21(9), 1146–1147. Retrieved from http://dx.doi.org/10.1038/s41593-018-0215-0
