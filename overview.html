<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Rick O. Gilmore" />


<title>Overview</title>

<script src="overview_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="overview_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="overview_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="overview_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="overview_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="overview_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="overview_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="overview_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="overview_files/navigation-1.1/tabsets.js"></script>
<script src="overview_files/navigation-1.1/codefolding.js"></script>
<link href="overview_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="overview_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Overview</h1>
<h4 class="author">Rick O. Gilmore</h4>
<h4 class="date">2019-06-28 15:30:48</h4>

</div>


<div id="purpose" class="section level1">
<h1><span class="header-section-number">1</span> Purpose</h1>
<p>To describe the BehAV.ai project team, resources available, and goals of the effort.</p>
</div>
<div id="support" class="section level1">
<h1><span class="header-section-number">2</span> Support</h1>
<div id="active" class="section level2">
<h2><span class="header-section-number">2.1</span> Active</h2>
<p>BehAV.ai is supported by the Penn State Institute for Cyberscience (ICS) seed grant program.</p>
<ul>
<li><a href="https://docs.google.com/document/d/1XwKC_F5byeeSgrrgL2t8wT7J4DDVVST1jY1Rdmr7NNg/edit?usp=sharing">Proposal narrative</a></li>
<li><a href="https://docs.google.com/document/d/13pungK4Vi6aOHCBQxBCmrkHJ3uUsyhgdRWLOzFyFHeg/edit?usp=sharing">Proposal abstract</a></li>
</ul>
</div>
<div id="potential" class="section level2">
<h2><span class="header-section-number">2.2</span> Potential</h2>
<ul>
<li><a href="https://grants.nih.gov/grants/guide/rfa-files/RFA-MH-19-145.html" class="uri">https://grants.nih.gov/grants/guide/rfa-files/RFA-MH-19-145.html</a></li>
<li><a href="https://grants.nih.gov/grants/guide/rfa-files/RFA-MH-19-147.html" class="uri">https://grants.nih.gov/grants/guide/rfa-files/RFA-MH-19-147.html</a></li>
</ul>
</div>
</div>
<div id="team-members" class="section level1">
<h1><span class="header-section-number">3</span> Team members</h1>
<ul>
<li>Dan Albohn, M.S., PSU (developer)</li>
<li>Kory Blose, Ph.D., PSU/ARL (Co-PI)</li>
<li>Stephen Fast, Ph.D., PSU/ARL (Co-PI)</li>
<li>Rick Gilmore, Ph.D., PSU/Databrary.org (PI)</li>
<li>Ori Ossmy, Ph.D., NYU (consultant)</li>
<li>Jeff Spies, Ph.D. Databrary.org (adviser)</li>
<li>Conrad Tucker, Ph.D., PSU/CMU (Co-PI)</li>
<li>Drew Polasky, PSU/ICS (consultant)</li>
</ul>
</div>
<div id="goals" class="section level1">
<h1><span class="header-section-number">4</span> Goals</h1>
<ol style="list-style-type: decimal">
<li>Implement video/image data analytic workflows on PSU HPC resources</li>
<li>Create UI to enable non-specialist users to deploy these workflows on their own videos</li>
<li>Demonstrate utility of using videos stored on Databrary or other API-accessible sources</li>
<li>Develop open/extensible video annotation format</li>
<li>(Stretch) pilot UI/UX for visualizing annotations from multiple video analysis tools</li>
<li>(Stretch) pilot scheme for semi-automated (human-in-the-loop) creation and modification of video analysis workflows</li>
</ol>
</div>
<div id="resources" class="section level1">
<h1><span class="header-section-number">5</span> Resources</h1>
<div id="technology-evaluation" class="section level2">
<h2><span class="header-section-number">5.1</span> Technology evaluation</h2>
<div id="domains" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Domains</h3>
<p>Most tools focus on a small subset of domains:</p>
<ol style="list-style-type: decimal">
<li>Object/face detection or labeling</li>
<li>Body pose estimation/motion tracking</li>
<li>Speech transcription</li>
</ol>
<p><a href="https://github.com/tyarkoni/pliers">Pliers</a>, on the other hand, provides a set of tools to generate annotations in a common data format.</p>
</div>
<div id="issues" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Issues</h3>
<ul>
<li>Open source vs. proprietary</li>
<li>Do cloud/AI as a service providers maintain confidentiality/privacy? Some terms of use involve giving unlimited rights to analytic providers.</li>
</ul>
</div>
<div id="software-stack" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Software stack</h3>
<p>Pliers, DeepLabCut, and LEAP are Python tools. VoTT uses node.js.</p>
<ul>
<li>A resource we may wish to consider is <a href="https://json-schema.org/">json-schema</a>.</li>
</ul>
</div>
</div>
<div id="open-source-software-tools" class="section level2">
<h2><span class="header-section-number">5.2</span> Open source software tools</h2>
<ul>
<li><a href="https://github.com/Microsoft/VoTT">Microsoft Visual Object Tagging Tool (VoTT)</a></li>
<li><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a> pose estimation</li>
<li><a href="https://github.com/AlexEMG/DeepLabCut">DeepLapCut</a> motion tracking, especially for non-human animals</li>
<li><a href="https://github.com/talmo/leap">Leap Estimates Animal Pose (LEAP)</a> - ROG met the developer, Talmo Pereira, at the 2019 BRAIN Initiative investigators meeting.</li>
<li><a href="http://jaaba.sourceforge.net/">JAABA - Janelia Automatic Animal Behavior Annotator</a></li>
<li><a href="https://github.com/tyarkoni/pliers">Pliers</a>, Python package for integrating video/audio/image feature extraction</li>
<li><a href="https://pjreddie.com/darknet/yolo/">You Only Look Once (Yolo)</a> object labeling
<ul>
<li>ROG’s colleague, Chen Yu, uses YOLO in his research.</li>
</ul></li>
<li><a href="https://opencv.org/">OpenCV</a></li>
</ul>
</div>
<div id="commercial-resources" class="section level2">
<h2><span class="header-section-number">5.3</span> Commercial resources</h2>
<ul>
<li><a href="https://www.ibm.com/us-en/marketplace/ibm-powerai-vision">IBM PowerAI Vision</a></li>
<li>IBM Intelligent Video Analytics</li>
<li><a href="https://www.microsoft.com/developerblog/2018/11/06/active-learning-for-object-detection/">Active Learning for Object Detection</a>.</li>
</ul>
</div>
<div id="bibliography" class="section level2">
<h2><span class="header-section-number">5.4</span> Bibliography</h2>
<p>Biggs, B., Roddick, T., Fitzgibbon, A., &amp; Cipolla, R. (2018, November 14). Creatures great and SMAL: Recovering the shape and motion of animals from video. arXiv [cs.CV]. Retrieved from arXiv.</p>
<p>Bornstein, A. (ari). (2019, February 4). Using Object Detection for Complex Image Classification Scenarios Part 4: Towards Data Science. Towards Data Science. Retrieved June 27, 2019, from <a href="https://towardsdatascience.com/using-object-detection-for-complex-image-classification-scenarios-part-4-3e5da160d272" class="uri">https://towardsdatascience.com/using-object-detection-for-complex-image-classification-scenarios-part-4-3e5da160d272</a>.</p>
<p>Clemens, J., Coen, P., Roemschied, F. A., Pereira, T. D., Mazumder, D., Aldarondo, D. E., Pacheco, D. A., et al. (2018). Discovery of a New Song Mode in Drosophila Reveals Hidden Structure in the Sensory and Neural Drivers of Behavior. Current biology: CB, 28(15), 2400–2412.e6. Elsevier. Retrieved from <a href="http://dx.doi.org/10.1016/j.cub.2018.06.011" class="uri">http://dx.doi.org/10.1016/j.cub.2018.06.011</a></p>
<p>Active Learning for Object Detection in Partnership with Conservation Metrics - Developer Blog. (2018, November 6). Developer Blog. Retrieved June 27, 2019, from <a href="https://www.microsoft.com/developerblog/2018/11/06/active-learning-for-object-detection/" class="uri">https://www.microsoft.com/developerblog/2018/11/06/active-learning-for-object-detection/</a></p>
<p>Francisco, F. A., Nührenberg, P., &amp; Jordan, A. L. (2019). A low-cost, open-source framework for tracking and behavioural analysis of animals in aquatic ecosystems. bioRxiv. biorxiv.org. Retrieved from <a href="https://www.biorxiv.org/content/10.1101/571232v1.abstract" class="uri">https://www.biorxiv.org/content/10.1101/571232v1.abstract</a></p>
<p>Gilmore, R.O. (n.d.). Databraryapi: An R package for interacting with the Databrary API. Retrieved from <a href="http://github.com/PLAY-behaviorome/databraryapi" class="uri">http://github.com/PLAY-behaviorome/databraryapi</a>.</p>
<p>Mathis, A., Mamidanna, P., Cury, K. M., Abe, T., Murthy, V. N., Mathis, M. W., &amp; Bethge, M. (2018). DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. Nature neuroscience, 21(9), 1281–1289. Retrieved from <a href="http://dx.doi.org/10.1038/s41593-018-0209-y" class="uri">http://dx.doi.org/10.1038/s41593-018-0209-y</a></p>
<p>Ossmy, O., Gilmore, R.O., &amp; Adolph, K.E. (in press). AutoViDev: A computer-vision framework to enhance and accelerate research in human development. In Advances in Intelligent Systems and Computing. Presented at the Computer Vision Conference (CVC) 2019, Springer.</p>
<p>Pereira, T. D., Aldarondo, D. E., Willmore, L., Kislin, M., -H. Wang, S. S., Murthy, M., &amp; Shaevitz, J. W. (2018, May 30). Fast animal pose estimation using deep neural networks. bioRxiv. Retrieved April 13, 2019, from <a href="https://www.biorxiv.org/content/10.1101/331181v2.abstract" class="uri">https://www.biorxiv.org/content/10.1101/331181v2.abstract</a></p>
<p>Prakash, S. K. A., &amp; Tucker, C. S. (2018). Bounded Kalman filter method for motion-robust, non-contact heart rate estimation. Biomedical Optics Express, 9(2), 873–897.</p>
<p>Štih, V., Petrucco, L., Kist, A. M., &amp; Portugues, R. (2019). Stytra: An open-source, integrated system for stimulation, tracking and closed-loop behavioral experiments. PLoS computational biology, 15(4), e1006699. journals.plos.org. Retrieved from <a href="http://dx.doi.org/10.1371/journal.pcbi.1006699" class="uri">http://dx.doi.org/10.1371/journal.pcbi.1006699</a></p>
<p>Wei, K., &amp; Kording, K. P. (2018). Behavioral tracking gets real. Nature neuroscience, 21(9), 1146–1147. Retrieved from <a href="http://dx.doi.org/10.1038/s41593-018-0215-0" class="uri">http://dx.doi.org/10.1038/s41593-018-0215-0</a></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
